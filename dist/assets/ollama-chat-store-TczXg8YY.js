import{p as c}from"./middleware-BFCXRDQO.js";import{d as h}from"./index-B_HhO1BD.js";const g=h()(c((t,d)=>({base64Images:null,chats:{},currentChatId:null,selectedModel:null,selectedProvider:"ollama",userName:"Imeso",isDownloading:!1,downloadProgress:0,downloadingModel:null,temperature:.7,topP:.9,maxTokens:1e6,batchSize:20,throttleDelay:50,systemPrompt:"You are a helpful AI assistant. Provide very long, detailed, and comprehensive responses using Markdown formatting. Include ALL relevant information from the provided context without exception. Do not omit any details, no matter how small or seemingly insignificant. Be thorough and exhaustive in your explanations. Structure your response with proper headings, lists, and formatting to ensure clarity and completeness. Always respond in the same language as the user's question. Prioritize completeness over brevity - longer responses are preferred. min length: 1000 words",chatMode:"general",selectedDataset:null,setBase64Images:e=>t({base64Images:e}),setUserName:e=>t({userName:e}),setCurrentChatId:e=>t({currentChatId:e}),setSelectedModel:e=>t({selectedModel:e}),setSelectedProvider:e=>t({selectedProvider:e}),getChatById:e=>d().chats[e],getMessagesById:e=>d().chats[e]?.messages||[],saveMessages:(e,o)=>{t(s=>{const a=s.chats[e];return{chats:{...s.chats,[e]:{messages:[...o],createdAt:a?.createdAt||new Date().toISOString()}}}})},handleDelete:(e,o)=>{t(s=>{const a=s.chats[e];if(!a)return s;if(o){const r=a.messages.filter(i=>i.id!==o);return{chats:{...s.chats,[e]:{...a,messages:r}}}}const{[e]:n,...l}=s.chats;return{chats:l}})},updateMessage:(e,o,s)=>{t(a=>{const n=a.chats[e];if(!n)return a;const l=n.messages.map(r=>r.id===o?{...r,content:s}:r);return{chats:{...a.chats,[e]:{...n,messages:l}}}})},startDownload:e=>t({isDownloading:!0,downloadingModel:e,downloadProgress:0}),stopDownload:()=>t({isDownloading:!1,downloadingModel:null,downloadProgress:0}),setDownloadProgress:e=>t({downloadProgress:e}),setTemperature:e=>t({temperature:e}),setTopP:e=>t({topP:e}),setMaxTokens:e=>t({maxTokens:e}),setBatchSize:e=>t({batchSize:e}),setThrottleDelay:e=>t({throttleDelay:e}),setSystemPrompt:e=>t({systemPrompt:e}),setChatMode:e=>t({chatMode:e}),setSelectedDataset:e=>t({selectedDataset:e})}),{name:"nextjs-ollama-ui-state",partialize:t=>({chats:t.chats,currentChatId:t.currentChatId,selectedModel:t.selectedModel,selectedProvider:t.selectedProvider,userName:t.userName,temperature:t.temperature,topP:t.topP,maxTokens:t.maxTokens,batchSize:t.batchSize,throttleDelay:t.throttleDelay,systemPrompt:t.systemPrompt,chatMode:t.chatMode,selectedDataset:t.selectedDataset})}));export{g as u};
